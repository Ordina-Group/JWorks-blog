---
layout: post
authors: [tim_vandendriessche, omer_tulumen, gabriel_dela_peña, oumaima_zerouali, viktor_vansteenweghen]
title: 'Cloudia'
image: /img/2024-03-05-cloudia/cloudia-banner.jpg
tags: [aws, terraform, java, python]
category: Cloud
comments: true
---

# Table of contents

- [Devcase Cloudia](#devcase-cloudia)
   * [Introduction](#introduction)
   * [Our Objective](#our-objective)
   * [Used Technologies](#used-technologies)
      + [Github Actions ](#github-actions)
      + [AWS](#aws)
      + [Terraform](#terraform)
      + [Docker](#docker)
      + [Spring Boot (Java)](#spring-boot-java)
      + [OpenAPI](#openapi)
      + [Gatling](#gatling)
   * [Architecture](#architecture)
- [AI](#ai)
   * [CodeWhisperer](#codewhisperer)
   * [Github Copilot](#github-copilot)
   * [ChatGPT](#chatgpt)
- [AWS](#aws-1)
   * [IoT core](#iot-core)
   * [Lambda](#lambda)
   * [S3](#s3)
   * [App runner](#app-runner)
   * [Elastic Container Registry (ECR)](#elastic-container-registry-ecr)
   * [Timestream Database](#timestream-database)
   * [Cloudwatch](#cloudwatch)
   * [AWS Api Gateway and OpenAPI](#aws-api-gateway-and-openapi)
   * [Route 53](#route-53)
   * [Cognito](#cognito)
- [Cloudia application](#cloudia-application)
   * [OBIS code data](#obis-code-data)
   * [Message converters](#message-converters)
      + [Java converter](#java-converter)
      + [Native image Java (GraalVM) converter](#native-image-java-graalvm-converter)
      + [Python converter](#python-converter)
   * [Benchmarking converters](#benchmarking-converters)
   * [Spring Security](#spring-security)
   * [Github Actions](#github-actions-1)
      + [Terraform run/publish application workflow](#terraform-runpublish-application-workflow)
      + [Dynamic deleting resources workflow](#dynamic-deleting-resources-workflow)
      + [Infracost workflow](#infracost-workflow)
      + [Scheduler ](#scheduler)
   * [Conclusion](#conclusion)

# Devcase Cloudia

## Introduction

In this blogpost we would like to share our project named "Cloudia". A project where a digital energy meter is being read with the help of a Raspberry PI and the output is being converted and stored in the AWS Cloud environment. In this blogpost we will talk about how this is done. We will talk about some AWS Services we will use and how the whole process is automated. We will also talk about security and monitoring implementations that have been implemented and much more! Hopefully you can enjoy reading our blogpost and maybe learn something new!

Happy reading!


## Our Objective

This project aims to convert raw data from digital meters into readable format and store it in both a time-series database (Timestream) on AWS and a NoSQL database (MongoDB). We also need to create an application that is deployed on AWS which will be used to makes requests on these data. Another objective is to benchmark different technologies such as python, Java and native Java (GraalVM) with converting raw data and saving it to the databases. In addition, the whole process should be automated and secure.

## Used Technologies

To successfully develop our project we used a number of technologies. In this section we provide an overview of these technologies and the reasoning behind them.

<img class="p-image" src="{{ '/img/2024-03-04-cloudia/overview_technologies_.png' | prepend: site.baseurl }}" class="image fit" style="margin:0px auto; max-width: 30%;">


### Github Actions 

To automate the whole process we need a CI/CD tool (continuous integration, continuous deployment). We have created multiple workflows in which each workflow has a different functionality. We will be diving into these workflows later on [Github actions](#github-actions-1). 

### AWS

AWS is the cloud provider that we used in our project. It has many services where each one has it's own usecase. We made use of some of these services in our project such as Apprunner, Lambdas, Cognito, etc. Later on in the blogpost we will take a closer look at the services that we have used.

### Terraform

Terraform is a Infrastructure as Code (IaC) tool by HashiCorp. It can be used to manage a cloud environment.
We have used this tool to set up our AWS cloud. Terraform uses its own declaritive language which means you describe the intended goal rather than the steps to reach that goal. It also allows for automating and provisioning cloud infrastructure.

### Docker

Docker is a popular tool to containerize applications for easier deployment, maintenance and management. We set up a Docker Image for our application which is then forwarded to AWS Elastic Container Registry (ECR).

### Spring Boot (Java)

Spring boot is a Java framework. We used Spring Boot 3 to create our application which is deployed on AWS later on.

### OpenAPI

OpenAPI is a handy tool when creating an API for your application. Both consumers and creators of the API are able to generate some code based on the specifications written down in the OpenAPI Document (OAD). This allows us the develop our codebase faster and more efficiently, and to keep it updated whenever our specification changes. Furthermore, consumers of the API can easily connect to it since they can similarly use the OAD to generate client side code.

A final useful feature of OpenAPI is that it allows many integrations, so we can automatically set up an API Gateway on AWS by making some minor changes to our OAD.

### Gatling

Since our project operates in the AWS cloud with the benefit of scalability (but also costs connected to upscaling), we decided to explore the effects of increased load / stress on the system to see when it would start failing.

Gatling is a popular tool for performing load testing because it can easily simulate a large number of concurrent users with a variety of simulated actions.

## Architecture

Now, let's talk about the architecture of our project.

<img class="p-image" src="{{ '/img/2024-03-04-cloudia/architecture.png' | prepend: site.baseurl }}" class="image fit" style="margin:0px auto;">

A digital electricity meter is being read using a raspberry pi. Raw data received from the electricity meter is published to an MQTT topic (message broker). We subscribe to this topic using IoT 
Core rules. AWS Lambda's get triggered by these rules when a new message is received. This lambda will convert this data and save it to the database.

We also have a Spring Boot application that makes requests to the database. This application is deployed using App runner. In order for a user to interact with the application, an api gateway and route 53 is used. To secure the apigateway and the application, Cognito is used. We will talk more about these services later on in the blog. 

We also need to automate the whole process so that new changes can be implemented very easily without manually updating resources. Also the resources should be created before work hours and get deleted after work hours (when it's not going to be used). We do all of this using Github Actions and Terraform. We will also talk about this later on in the blog. 

# AI

During the development of this project we explored a few AI tools to assist us with various tasks. We explored the possibilities for coding in a variety of scripting and programming languages to see whether these tools help us with greater productivity, increased development speed and better bug searching. Some of these tools can generate and autocomplete code. Furthermore, they claim to help with code refactoring and are often integrated with IDEs to help diagnose and fix bugs.

## CodeWhisperer

Amazon CodeWhisperer is an open-source development tool that facilitates efficient collaboration and communication among software development teams. It aims to improve the overall development proces by providing a platform for sharing knowledge, code, best practices and ultimately leading to better code quality and faster development cycles. CodeWhisperer enables developers to work more cohesively and make informed decisions during the software development lifecycle. It fosters an environment where code reviews, feedback and discussions can happen seamlessly within the development team.

Amazon CodeWhisperer provides code generation capabilities for various programming languages and technologies. However, this is still quite limited at the moment. For example, it does not yet provide support for YAML and Terraform.

In our project we explored it mainly for Java which has mixed results. Amazon CodeWhisperer does generate code, but is notably slow and may not even produce the desired output. For example, an attempt to generate attributes for a specific class may result in unusual attributes being generated. Additionally, manual intervention is needed since code generation for endpoints can be somewhat inconsistent.

In conclusion, you cannot rely fully on CodeWhisperer for Java development, but it may augment your existing code to some degree.

## Github Copilot

GitHub Copilot is another AI-powered code completion tool. It was developed from a collaboration between GitHub and OpenAI. It's designed to assist developers by suggesting code whilst they write. Copilot works as a plugin within various IDE's, such as IntelliJ and Visual Studio Code.

In our experience with Copilot, suggestions take quite long to appear and Copilot fails to understand the context of our code beyond classes.
For instance, when creating a 'Person' class with a name and birthdate and asking Copilot to write tests for this class.
The test should be solely for the name and birthdate but the generated code also included other fields that do not exist on the class, such as phone number and gender of the person. 

On the other hand, Github Copilot excelled at writing small functions, such as calculating the sum of two numbers or checking if a number is prime.

In conclusion, for applications that don't have a high complexity, Github Copilot is suitable. The more complex the application gets, the more likely you will encounter problems with this tool.

## ChatGPT

We also explored ChatGPT during our project. It has proven to be useful in some cases but not always.
Oftentimes, we provide code as input and chatGPT can help us locate issues, modify the code to different formats and give suggestions when we have bugs and so on.

Sometimes it's a wolf in sheeps clothing since suggestions and ideas proposed may simply be wrong, outdated or incompatible whilst ChatGPT insists that it is correct. However, it proves to be useful in many ways as long as you keep in mind the drawbacks and imperfections. If issues persist, have a look elsewhere in case the tool is outdated or wrongly informed. Oftentimes it helps provide an easy way to modify code or get started with something new.

# AWS

Now let's go more in depth into the AWS services that have been used in our project.

## IoT core

In order to retrieve smart meter data, IoT Core service has been used. This service let's you send messages (data) from IoT devices to an MQTT topic. In order to access this data, you have to subscribe to the same MQTT topic. This has been done using the IoT message routing rules. AWS Lambda's can use these rules as triggers, so whenever a new message is sent to a topic, the lambda in which the rule is attached will run with the provided message.

## Lambda

AWS Lambda is a serverless service that lets you run code for virtually any type of application or backend service without managing servers. You can trigger Lambda from over 200 AWS services and software as a service (SaaS) applications, and only pay for what you use.

We created multiple Lambda functions to process raw data (OBIS code) returned by a digital meter. These lambdas were implemented in Python, Java and in native Java (GraalVM). Each Lambda function converts the raw data into JSON format and stores it in either Timestream or MongoDB.

The image below displays the result of converting OBIS code data to JSON format using a Lambda function and storing it in a Timestream database.

<img class="p-image" src="{{ '/img/2024-03-04-cloudia/converted_obis_ts.png' | prepend: site.baseurl }}" class="image fit" style="margin:0px auto; max-width: 30%;">

## S3

Amazon S3 (Simple Storage Service) is like the name suggests, a storage service where you can store all kinds of data. We use S3 for our Terraform state so the current state of the AWS resources are stored. So this state will be compared with the incoming state when making changes on the cloud. We also make use of S3 to store native images (GraalVM). A zip file of the native image of our application is uploaded on the S3 which will then be used to deploy native lambda's.

## App runner

App runner is another AWS service which is a fully managed application that lets you build, deploy and run web applications. It's an easy way to deploy an application on AWS. AWS handles configurations such as load balancing and automatical scaling. In our project we push our image of our application to [ECR](#elastic-container-registry-ecr). The App runner detects incoming images and updates the current deployed application with a new version.

In Terraform we provide some configuration to deploy our application on App runner. In here the ECR registry is provided. Auto deployment is enabled in here as well so new versions of the application can be detected and update the App runner.

## Elastic Container Registry (ECR)

The ECR is a container service which allows us to use Docker Images in the AWS Cloud environment. Our application is pushed to ECR. Using Terraform we set up our registry:

```javascript
resource "aws_ecr_repository" "my_repo" {
  name                 = "cloudia"
  image_tag_mutability = "MUTABLE"

  image_scanning_configuration {
    scan_on_push = true
  }
}
```

This registry is used by the AWS App Runner which hosts our backend for the API Gateway.

## Timestream Database

AWS Timestream is a tool to manage and track time series data. Time series data consists of a collection of data points or observations recorded at specific time intervals. Each data point in a time series is associated with a timestamp, indicating when the observation was made. The data values can represent various types of information such as stock prices, temperature readings or any other variable that change over time. In our case, we are storing time series data from a digital meter such as energy production, energy consumption, average consumption and more.

## Cloudwatch

Amazon CloudWatch is a service that lets you monitor AWS services. It gives visibility into performance of AWS resources and allows users to set alarms, automatically react to changes and gains a unified view of operational health.

CloudWatch is used to monitor Lambda functions including tracking durations, errors, and invocations. Other AWS resources such as [App runner](#app-runner) is also monitored using cloudwatch logs. In order for cloudwatch to work, the correct permissions has to be set for the created resources.

The following image illustrates a section of the AWS CloudWatch dashboard that has been implemented.

<img class="p-image" src="{{ '/img/2024-03-04-cloudia/cloudwatch_dashboard.png' | prepend: site.baseurl }}" class="image fit" style="margin:0px auto; max-width: 30%;">

## AWS Api Gateway and OpenAPI

The API Gateway connects our API with the outside world such that applications can consume our API (if authorized).
Using a Terraform script we can create and tear down our API Gateway easily. Furthermore, our setup uses an OpenAPI Document (OAD) as input to generate the required endpoints and their integration with other AWS services.

The OAD describes our API in detail and has multiple features. Firstly, it serves as documentation of the API. Secondly, it allows backend code generation within our project to configure the API controllers. Furthermore, it can be used by third parties to generate code in their own frontend applications to call and interact with our API. And finally, to set up our apigateway with Terraform, OAD is used to configure lambda integrations and cognito.

Our OAD contains multiple sections. Most crucial for our discussion here is the 'paths' segment in which we describe the endpoints of our API and the associated methods (GET, POST, PUT, DELETE). As an example we have our endpoint `/timestream/betweenTwoTimestamp` with a `GET` method attached to it.

Using the `security` property we configure the endpoint to be set up according to our configuration in a specific Cognito User Pool. Via the `x-amazon-apigateway-integration` property we configure the API Gateway for this specific API endpoint.

````yaml
  /timestream/betweenTwoTimestamp:
    get:
      security:
        - CognitoUserPool: [ "cloudia/all" ]
      summary: Fetch data between two specific timestamp
      x-amazon-apigateway-integration:
        type: HTTP_PROXY
        httpMethod: GET
        uri: 'https://${apprunner_url}/timestream/betweenTwoTimestamp'
        payloadFormatVersion: 2.0
      operationId: getBetweenTwoTimestampTimestream
````

A few other properties are ommited from our screenshot; we use the `parameters` property to set the request parameters of the API and we use the `responses` property to define potential responses that can be received back by the end user.

Below is an example of our AWS Api Gateway endpoints on AWS.

<img class="p-image" src="{{ '/img/2024-03-04-cloudia/apigateway.png' | prepend: site.baseurl }}" class="image fit" style="margin:0px auto; max-width: 30%;">

## Route 53

Route 53 is a `Domain Name System` (DNS) service. It lets you route requests to your domain. In our project it is used with the [api gateway](#apigateway-and-openapi). User's are able to make requests to the route 53 endpoint which will then forward it to the api gateway and resolve the request.

## Cognito

Cognito is an identity and access management service by AWS. This service provides `security` to our application (using Spring security) as well as for our AWS API Gateway. The user has to login in order to access these resources and thus provide an access token. There are multiple ways in order to retrieve the access token. In our project we chose the `Authorize Code Grant` flow. This flow will retrieve an authorization code first which then will be used to request an access token. In order to make requests to the AWS API Gateway and/or apprunner, this access token has to be provided in order to get access.

# Cloudia application

## OBIS code data

OBIS code data are standardized identifiers used by P1 meters (Dutch Digital Electricity Meter) to represent different types of energy consumption data. Each OBIS code identifier corresponds to a specific piece of information, such as electricity usage, meter status, or tariff information. These codes help organize and categorize the data collected by the meter. Below snippet code is an example:

````
0-0:96.1.4(xxxxx)
0-0:96.1.1(xxxxxxxxxxxxxxxxxxxxxxxxxxxx)
0-0:1.0.0(210204163628W)
1-0:1.8.1(000439.094kWh)
1-0:1.8.2(000435.292kWh)
1-0:2.8.1(000035.805kWh)
1-0:2.8.2(000012.156kWh)
0-0:96.14.0(0001)
````
For more details about OBISCODE, please refer to the following [Link](https://jensd.be/1205/linux/data-lezen-van-de-belgische-digitale-meter-met-de-p1-poort).

## Message converters

The data generated by the P1 Meter is in the form of OBIS code data, as previously explained. To improve readability, a script is needed to translate this data into a readable format. As a result, we have developed multiple Lambdas for this purpose, using Java, Python, and Native implementations. The objective is to benchmark these Lambdas to determine which one is the most optimal in terms of performance, price, speed, and relevance for our project. These Lambdas will receive OBIS code data and convert it into JSON formatted data. Afterwards, this data will be stored in a database, such as AWS Timestream or MongoDB. In the following sections, the process of creating these Lambdas will be explained.

### Java converter

OBIS code data is converted into JSON format. It processes each OBIS code and maps it to the object. The following is a sample illustration how this is done.

```java
        switch (obisCode) {
            case "0-0:96.1.4":
                data.setId(value);
                break;
            case "0-0:96.1.1":
                data.setSerialNumberElectricityMeter(value);
                break;
            case "0-0:1.0.0":
                String timestampString = value.substring(0, value.length() - 1) + "Z";
                Instant timestamp = Instant.from(formatter.parse(timestampString));
                data.setTimestamp(timestamp);
                break;
                // rest of the fields ...
```

### Native image Java (GraalVM) converter

A native image is a tool that compiles the code ahead-of-time. This means it will try to compile everything ahead-of-time instead of runtime. It also creates an executable instead of a jar file when compiling the code. It can make the application faster because the compilation happens ahead-of time which enhances the user experience. However, it does come with some cons aswell. The compiling of the application takes much longer then without the use of native images. Because it's compiles ahead-of-time, classes that use reflection for example, can also cause problems. So there are sometimes compatability issues with it which can bring more complexity to the code.

### Python converter

We've also developed the message converter using Python script. This is done to benchmark both languagues. In the following chapter, you can find more information about the results of the benchmarking.

```python
            if key == '0-0:1.0.0':
                timestamp_str = value.replace('W', 'Z')
                input_format = '%y%m%d%H%M%S%z'  
                timestamp_datetime = datetime.strptime(timestamp_str, input_format).isoformat()               
                dataObject['timestamp'] = timestamp_datetime           
            elif key == '0-0:96.1.4':
                dataObject['meterId'] = value                           
            elif key == '0-0:96.1.1':
                dataObject['serialNumberElectricityMeter'] = value
            # rest of the fields ...
```

## Benchmarking converters

Multiple Lambdas have been created, and benchmarking on each Lambda has been performed to determine which is the most optimal in terms of performance, price, speed, and relevance for our project. Each Lambda was run separately, and the following images display the results of the invocations for each one.

Below are the results for the Java Lambda Converter.

<img class="p-image" src="{{ '/img/2024-03-04-cloudia/benchmark-java.png' | prepend: site.baseurl }}" class="image fit" style="margin:0px auto; max-width: 30%;">

Below are the results for the Python Lambda Converter.

<img class="p-image" src="{{ '/img/2024-03-04-cloudia/benchmark-python.png' | prepend: site.baseurl }}" class="image fit" style="margin:0px auto; max-width: 30%;">

Below are the results for the Native Lambda Converter.

<img class="p-image" src="{{ '/img/2024-03-04-cloudia/benchmark-native.png' | prepend: site.baseurl }}" class="image fit" style="margin:0px auto; max-width: 30%;">

Based on the displayed screenshots, it's clear that the Python converter outperforms the others overall. With an init duration of 56.88 ms.
This is because Python is a scripting language that requires minimal libraries to install, resulting in a lower billed duration, The total duration of 406.54 ms indicates rapid execution.
Following closely is the native converter, ranking as the second-best option overall. It has an init duration of 1720.04 ms and a billed duration of 3517 ms, with a total duration of 1796.90 ms. 
The maximum memory used here is slightly higher at 128 MB. The Java converter follows, with an init duration of 6770.41 ms. 
Java's need to install libraries before execution directly impacts its longer initialization duration. 
This leads to a billed duration of 6081 ms and a total duration of 6080.41 ms. Similar to the native converter, the maximum memory used by the Java converter is 128 MB.

## Spring Security

Spring security has been implemented to have security enabled on our application. The application will check if the user is authenticated or not by checking and validating an access token. To implement Spring security in Spring Boot 3 we have provided a SecurityFilterChain. Using this filterchain configuration, you can configure which routes should be authenticated and how. In our project, we wanted a simple authentication process which checks if a valid access token is provided. We do this using the following code.

```java
    @Bean
    public SecurityFilterChain filterChain(HttpSecurity http) throws Exception {

        http
                .authorizeHttpRequests((requests) -> requests
                        .requestMatchers("/mongodb/**").authenticated()
                        .requestMatchers("/timestreamdb/**").authenticated()
                        .anyRequest().permitAll()
                )
                .oauth2ResourceServer(oauth2 -> oauth2.jwt(Customizer.withDefaults()))
                .httpBasic(withDefaults());
        return http.build();
    }

    @Bean
    public JwtDecoder jwtDecoder() {
        return JwtDecoders.fromIssuerLocation(issuerUri);
    }

```

As you can see, the routes to mongodb and timestreamdb needs authentication. Because we use Cognito oauth2, we want to validate access tokens. Therefore we have to let Spring know that it should also act as a resource server. Lastly, in order to validate the access token, we use the jwtDecoder. This is needed in order for Spring to decode the jwt token. 

The last thing we have to provide to Spring is details about your Cognito configuration in the application.properties file. The client ID and client secret of your Cognito service configuration have to be provided in order for Spring to know where to authenticate against and authorize requests with Amazon Cognito. Additionally, the issuer URI needs to be specified to validate tokens received from Cognito. This URI is used to verify the authenticity of the tokens.

```java
spring.security.oauth2.client.registration.cognito.client-id=${CLIENT_ID}
spring.security.oauth2.client.registration.cognito.client-secret=${CLIENT_SECRET}
spring.security.oauth2.client.provider.cognito.issuer-uri=https://cognito-idp.eu-west-1.amazonaws.com/${USER_POOL_ID}
```

## Github Actions

We don't want to manually deploy our application and manually run terraform. This whole process is automated using Github Actions workflows. These are CI/CD pipelines. These pipelines consists of one or more jobs. Jobs are a set of steps that are going to be executed. Let's take a look at some of these workflows we have configured.

### Terraform run/publish application workflow

The first workflow we're going talk about is the workflow where we publish the application and run our Terraform. To run Terraform, we run the command `Terraform apply`. This command will apply all the changes of the Terraform file.

````yaml
      - name: Terraform Apply
        id: apply
        run: terraform apply -auto-approve 
````

In this workflow we also want to push our dockerized application to ECR. To do this we need to first build docker image of our application and push it to ECR. 

### Dynamic deleting resources workflow

At the end on the workday, we want to delete all the resources we had created. Of course on a production environment, this wouldn't be done. 
But to be efficient with our resources, we delete them at the end of the workday for our development environment. 
Since it is possible that someone works in the late evening on the project, we have configured a Telegram notification. 
Before destroying the resources, a message is sent to a telegram channel asking whether it's okay to destroy the resources. 
If it doesn't get a response, the resources get destroyed. If someone responds with a "no", then the deletion gets delayed with 2 hours.

````yaml
      ...
      - name: Terraform Destroy
        id: destroy
        run: terraform destroy -auto-approve
        continue-on-error: true
````
### Infracost workflow

Another feature that has been added is the implementation of Infracost. 
Infracost is a tool that can be used to monitor the cost of the resources which are going to be created with Terraform. 
It compares the previous Terraform state with the current one. 
Whenever a pull request is made to the production environment, the infracost workflow will run and provide comment a report of the changes in the infrastructure cost that are going to happen if applied.

The comment on the PR looks as follows:

<img class="p-image" src="{{ '/img/2024-03-04-cloudia/infracost.png' | prepend: site.baseurl }}" class="image fit" style="margin:0px auto; max-width: 30%;">

### Scheduler 

Now we also want to deploy and provision these AWS resources during work hours. This is done using scheduler. Whenever a workflow needs to be runned on a specific time, the scheduler can be added to do this.

In the following screenshot an example is shown how to configure the scheduler on a workflow. 

````yaml
on:
    push:
        branches: [ "main" , "dev"]

#      ┌───────────── minute (0 - 59)
#      │ ┌───────────── hour (0 - 23)
#      │ │ ┌───────────── day of the month (1 - 31)
#      │ │ │ ┌───────────── month (1 - 12 or JAN-DEC)
#      │ │ │ │ ┌───────────── day of the week (0 - 6 or SUN-SAT)
#      │ │ │ │ │
#      │ │ │ │ │
#      │ │ │ │ │
#      * * * * *
    schedule:
    - cron: '0 7 * * 1-5'
````

## Conclusion

In summary, a P1 digital electricity meter is being read and sent to an MQTT topic. We have multiple lambda's that gets triggered when a new message (data) is sent to this topic. The lambda's convert and save this data to the database(s). 

We have multiple lambda's for Java, Java native (GraalVM) and python. They do all the exact same thing which is to convert data and save it to either Timestream database or MongoDB database.

The benchmarking conclusions for these lambdas are the following:
the Python Lambda converter has the best performance in speed, memory usage, and billing efficiency.
Following closely is the Native Lambda converter, which also demonstrates good overall performance but falls slightly short compared to the Python Lambda.
Lastly, the Java Lambda converter ranks lowest in overall performance compared to the other options.

Now we also have deployed our application on AWS App runner which is also configured with Spring Security. The application interacts with the databases. To interact with the AWS App Runner, API Gateway and Route 53 has been configured. We have also implemented security (Cognito) on the API Gateway. A user can make requests to the domain hosted on Route 53. These requests are then routed through the AWS API Gateway, which subsequently forwards them to the AWS App Runner for processing. Only authenticated users can make these requests to the API Gateway.

The whole process has also been automated using Terraform and CI/CD pipeline which makes it much easier to change our application and/or AWS resources. 

Lastly, we would like to talk about the AI tools we have experimented with. It is still early for many AI tools to help with code development and bug fixing. However, this seems promising for the future and already some tools can augment us in multiple ways. Each AI tool has some interesting features that it provides. In our experience ChatGPT was the most promising. However, all of these tools did come with caviats. As for now, there might still be progress to be made regarding to AI tools but it looks like it does have a bright future ahead.