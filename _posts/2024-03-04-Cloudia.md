---
layout: post
authors: [tim_vandendriessche, omer_tulumen, gabriel_dela_pe√±a, oumaima_zerouali, viktor_vansteenweghen]
title: 'Cloudia'
image: /img/2024-03-05-cloudia/cloudia-banner.jpg
tags: [aws, terraform, java, python]
category: Cloud
comments: true
---

<img class="p-image" src="{{ '/img/2024-03-04-cloudia/AFBEELDING.png' | prepend: site.baseurl }}" class="image fit" style="margin:0px auto; max-width: 30%;">

NOTITIES:

S3 uitleggen,ook uitleggen in terraform en native.
# Table of contents

- [Devcase Cloudia](#devcase-cloudia)
    * [Introduction](#introduction)
    * [Our Objective](#our-objective)
    * [Journey and Challenges](#journey-and-challenges)
    * [Used Technologies](#used-technologies)
        + [Github Actions (**********)](#github-actions-)
        + [AWS](#aws)
        + [Terraform](#terraform)
        + [Docker](#docker)
        + [Spring Boot (Java)](#spring-boot-java)
        + [OpenAPI](#openapi)
        + [Postman](#postman)
        + [Gatling](#gatling)
        + [Renovate](#renovate)
    * [Architecture](#architecture)
- [Devops](#devops)
    * [Github Actions](#github-actions)
        + [Workflow A](#workflow-a)
        + [Workflow B](#workflow-b)
        + [Workflow C](#workflow-c)
- [AWS ](#aws-1)
    * [Lambda](#lambda)
    * [Timestream Database ](#timestream-database)
    * [Cloudwatch](#cloudwatch)
    * [Apigateway and OpenAPI](#apigateway-and-openapi)
    * [Elastic Container Registry (ECR)](#elastic-container-registry-ecr)
    * [Route 53](#route-53)
    * [Cognito](#cognito)
    * [App runner](#app-runner)
    * [IoT](#iot)
- [Cloudia application](#cloudia-application)
    * [End Result](#end-result)
    * [Developer Experience](#developer-experience)

# Devcase Cloudia

## Introduction

In this blogpost we would like to share our project named "Cloudia". A project where a digital energy meter is being read with the help of a Raspberry PI and the output is being converted and stored in the AWS Cloud environment. In this blogpost we will talk about how this is done. We will talk about some AWS Services we will use and how the whole process is automated. We will also talk about security and monitoring implementations that have been implemented and much more! Hopefully you can enjoy reading our blogpost and maybe learn something new!

Happy reading!


## Our Objective

This project aims to convert raw data from digital meters into readable format and store it in both a time-series database (Timestream) on AWS and a NoSQL database (MongoDB). We also need to create an application that is deployed on AWS which will be used to makes requests on these data. Another objective is to benchmark different technologies such as python, Java and native Java (GraalVM) with converting raw data and saving it to the databases. In addition, the whole process should be automated and secure.

## Used Technologies

To successfully develop our project we used a number of technologies. In this section we provide an overview of these technologies and the reasoning behind them.

<img class="p-image" src="{{ '/img/2024-03-04-cloudia/overview_technologies_.png' | prepend: site.baseurl }}" class="image fit" style="margin:0px auto; max-width: 30%;">


### Github Actions 

To automate the whole process we need a CI/CD tool (continuous integration, continuous deployment). We have created multiple workflows in which each workflow has a different functionality. We will be diving into these workflows later on. --> REFERENCE TO HYPERLINK

### AWS

AWS is the cloud provider that we used in our project. It has many services where each one has it's own usecase. We made use of some of these services in our project such as Apprunner, Lambdas, Cognito, etc. Later on in the blogpost we will take a closer look at the services that we have used.

### Terraform

Terraform is a Infrastructure as Code (IaC) tool by HashiCorp. It can be used to manage a cloud environment.
We have used this tool to set up our AWS cloud. Terraform uses its own declaritive language which means you describe the intended goal rather than the steps to reach that goal. It also allows for automating and provisioning cloud infrastructure.

### Docker

Docker is a popular tool to containerize applications for easier deployment, maintenance and management. We set up a Docker Image for our application which is then forwarded to AWS Elastic Container Registry (ECR).

### Spring Boot (Java)

Spring boot is a Java framework. We used Spring Boot 3 to create our application which is deployed on AWS later on.

### OpenAPI

OpenAPI is a handy tool when creating an API for your application. Both consumers and creators of the API are able to generate some code based on the specifications written down in the OpenAPI Document (OAD). This allows us the develop our codebase faster and more efficiently, and to keep it updated whenever our specification changes. Furthermore, consumers of the API can easily connect to it since they can similarly use the OAD to generate client side code.

A final useful feature of OpenAPI is that it allows many integrations, so we can automatically set up an API Gateway on AWS by making some minor changes to our OAD.

### Postman

To test the endpoints for our application API's, we used Postman. It is a popular tool that can send HTTP requests (similar to a browser, but without rendering HTML web pages) and receive responses. This made it easier and faster to test and develop our API's, including the security and authorization headers.

### Gatling

Since our project operates in the AWS cloud with the benefit of scalability (but also costs connected to upscaling), we decided to explore the effects of increased load / stress on the system to see when it would start failing.

Gatling is a popular tool for performing load testing because it can easily simulate a large number of concurrent users with a variety of simulated actions.

### Renovate

When deploying applications, it is of utmost importance to update dependencies regularly and securely. Renovate is a useful tool to automatically notify us when our dependencies have a newer version by setting up a pull request to prepare the update.

## Architecture

Now, let's talk about the architecture of our project.

<img class="p-image" src="{{ '/img/2024-03-04-cloudia/architecture.png' | prepend: site.baseurl }}" class="image fit" style="margin:0px auto; max-width: 30%;">

A digital electricity meter is being read using a raspberry pi. Raw data received from the electricity meter is published to an MQTT topic (message broker). We subscribe to this topic using IoT 
Core rules. AWS Lambda's get triggered by these rules when a new message is received. This lambda will convert this data and save it to the database.

We also have a Spring Boot application that makes requests to the database. This application is deployed using App runner. In order for a user to interact with the application, an api gateway and route 53 is used. To secure the apigateway and the application, Cognito is used. We will talk more about these services later on in the blog. 

We also need to automate the whole process so that new changes can be implemented very easily without manually updating resources. Also the resources should be created before work hours and get deleted after work hours (when it's not going to be used). We do all of this using Github Actions and Terraform. We will also talk about this later on in the blog. 

# AWS

Now let's go more in depth into the AWS services that have been used in our project.

## IoT core

In order to retrieve smart meter data, IoT Core service has been used. This service let's you send messages (data) from IoT devices to an MQTT topic. In order to access this data, you have to subscribe to the same MQTT topic. This has been done using the IoT message routing rules. AWS Lambda's can use these rules as triggers, so whenever a new message is sent to a topic, the lambda in which the rule is attached will run with the provided message.

## Lambda

AWS Lambda is a serverless service that lets you run code for virtually any type of application or backend service without managing servers. You can trigger Lambda from over 200 AWS services and software as a service (SaaS) applications, and only pay for what you use.

We created multiple Lambda functions to process raw data (OBIS code) returned by a digital meter. These lambdas were implemented in Python, Java and in native Java (GraalVM). Each Lambda function converts the raw data into JSON format and stores it in either Timestream or MongoDB.

Below is a snippet of Java code representing a Lambda function. It receives OBIS code data in Base64 format from the [IoT Core](#iot-core) service. Then converts this data to JSON format and stores it in a timestream database.

```java
    @Bean
    public Function<String, String> convertAndInsertDataToTimestreamDb() {
        return responseTopic -> {
            try {
                ObjectMapper objectMapper = new ObjectMapper();

                TopicResponse topicResponse = objectMapper.readValue(responseTopic, TopicResponse.class);
                String rawSmartMeterDataBase64 = topicResponse.getBase64OriginalPayload();

                byte[] decoded = Base64.getDecoder().decode(rawSmartMeterDataBase64);
                String rawSmartMeterData = new String(decoded, StandardCharsets.UTF_8);
                List<SmartMeterData> convertedSmartMeterDataList = converter.convertRawSmartMeterDatasToSmartMeterDataList(rawSmartMeterData);

                return timestreamService.insertDataToTimestream(convertedSmartMeterDataList);
            } catch (IOException e) {
                throw new RuntimeException("Error inserting data", e);
            }
        };
    }
```

These lambda functions are created using Terraform as demonstrated below.

```javascript
resource "aws_lambda_function" "smartmeterdata_converter_and_save_to_timestreamdb" {
  function_name = "java_convert_rawSmartMeterData_and_save_to_timestream_v1"
  runtime       = "java17"
  handler       = "org.springframework.cloud.function.adapter.aws.FunctionInvoker"
  timeout       = 30
  role          = var.lambda_timestream_role_arn
  filename      = "${path.root}/../lambdas/target/lambdas-0.0.1-SNAPSHOT-aws.jar" # Update with your actual JAR file path

  environment {
    variables = {
      SPRING_PROFILES_ACTIVE = "cloud"
      SPRING_APPLICATION_JSON = jsonencode({
        "spring.cloud.function.definition" = "convertAndInsertDataToTimestreamDb"
      })
    }
  }
}
```

The image below displays the result of converting OBIS code data to JSON format using a Lambda function and storing it in a Timestream database.

## S3

DIT NOG DOEN

## App runner

App runner is another AWS service which is a fully managed application that lets you build, deploy and run web applications. It's an easy way to deploy an application on AWS. AWS handles configurations such as load balancing and automatical scaling. In our project we push our image of our application to [ECR](#elastic-container-registry-ecr). The App runner detects incoming images and updates the current deployed application with a new version.

In Terraform we provide some configuration to deploy our application on App runner. In here the ECR registry is provided. Auto deployment is enabled in here as well so new versions of the application can be detected and update the App runner.

```` javascript
resource "aws_apprunner_service" "apprunner_service" {
  depends_on = [
    aws_iam_role_policy_attachment.attach_ecr_policy,
    aws_iam_role_policy_attachment.attach_cloudwatch_policy,
    aws_iam_role_policy_attachment.attach_timestream_policy,
    time_sleep.wait_role_to_create
  ]
  service_name = "cloudia-apprunner-test-v1"

  instance_configuration {
    instance_role_arn = aws_iam_role.iam_for_apprunner_timestream_instance_role.arn
  }

  source_configuration {
    auto_deployments_enabled = true

    image_repository {
      image_configuration {
        port = "8080"
        runtime_environment_variables = {
          MONGODB_PASSWORD = var.mongodb_password
          CLIENT_ID        = var.cognito_client.id
          CLIENT_SECRET    = var.cognito_client.client_secret
          USER_POOL_ID     = var.user_pool.id
        }
      }

      image_identifier      = "${var.ecr_repo.repository_url}:latest"
      image_repository_type = "ECR"
    }

    authentication_configuration {
      access_role_arn = aws_iam_role.iam_for_apprunner.arn
    }
  }
}

resource "time_sleep" "wait_role_to_create" {
  depends_on      = [aws_iam_role.iam_for_apprunner]
  create_duration = "10s"
}

````

## Elastic Container Registry (ECR)

The ECR is a container service which allows us to use Docker Images in the AWS Cloud environment. Our application is pushed to ECR. Using Terraform we set up our registry:

```javascript
resource "aws_ecr_repository" "my_repo" {
  name                 = "cloudia"
  image_tag_mutability = "MUTABLE"

  image_scanning_configuration {
    scan_on_push = true
  }
}
```

This registry is used by the AWS App Runner which hosts our backend for the API Gateway.

## Timestream Database

AWS Timestream is a tool to manage and track time series data. Time series data consists of a collection of data points or observations recorded at specific time intervals. Each data point in a time series is associated with a timestamp, indicating when the observation was made. The data values can represent various types of information such as stock prices, temperature readings or any other variable that change over time. In our case, we are storing time series data from a digital meter such as energy production, energy consumption, average consumption and more.


## Cloudwatch

Amazon CloudWatch is a service that lets you monitor AWS services. It gives visibility into performance of AWS resources and allows users to set alarms, automatically react to changes and gains a unified view of operational health.

CloudWatch is used to monitor Lambda functions including tracking durations, errors, and invocations. Other AWS resources such as [App runner](#app-runner) is also monitored using cloudwatch logs. In order for cloudwatch to work, the correct permissions has to be set for the created resources.

The snippet below displays how a single metric is created using Terraform. In our project, for each lambda these metrics are being displayed.

```javascript
errorsTitle = flatten([
    {
      type = "text",
      #      x      = 0,
      #      y      = 0,
      width  = 24,
      height = 2,
      properties = {
        "markdown" : "## Errors",
        "markdown_size" : "small",
        "background" : "transparent"
      }
    }
  ])

  errors = flatten([
    for i, lambda_name in local.lambdas : [
      {
        type = "metric"
        #        x      = 0
        #        y      = 6 * (i == 0 ? 1 : i * 6)
        width  = 8
        height = 6
        properties = {
          view    = "timeSeries"
          stacked = false
          metrics = [["AWS/Lambda", "Errors", "FunctionName", lambda_name]]
          region  = "eu-west-1"
          title   = lambda_name
        }
      },
    ]
  ])

  errorConcatted = flatten(concat(local.errorsTitle, local.errors))
  ```

In the AWS CloudWatch dashboard, the new metric for errors is displayed.

## AWS Api Gateway and OpenAPI

The API Gateway connects our API with the outside world such that applications can consume our API (if authorized).
Using a Terraform script we can create and tear down our API Gateway easily. Furthermore, our setup uses an OpenAPI Document (OAD) as input to generate the required endpoints and their integration with other AWS services.

The OAD describes our API in detail and has multiple features. Firstly, it serves as documentation of the API. Secondly, it allows backend code generation within our project to configure the API controllers. Furthermore, it can be used by third parties to generate code in their own frontend applications to call and interact with our API. And finally, to set up our apigateway with Terraform, OAD is used to configure lambda integrations and cognito.

```` javascript
resource "aws_api_gateway_rest_api" "cloudia_api_gateway_openapi" {
  body = data.template_file.openapi_yml.rendered
  name = "cloudia-openapi-gateway-v1"

  endpoint_configuration {
    types = ["REGIONAL"]
  }
}

data "template_file" "openapi_yml" {
  template = file("${path.root}/../open-api/src/main/resources/swagger/openapi.yml")
  vars = {
    apprunner_url                            = aws_apprunner_service.apprunner_service.service_url
    user_pool_id                             = var.user_pool.id
    python_timestream_lambda_invoke_arn      = var.lambda_python_timestream.invoke_arn
    python_mongodb_lambda_invoke_arn         = var.lambda_python_mongodb.invoke_arn
    java_timestream_lambda_invoke_arn        = var.lambda_java_timestream.invoke_arn
    java_mongodb_lambda_invoke_arn           = var.lambda_java_mongodb.invoke_arn
    native_java_timestream_lambda_invoke_arn = var.native_lambda_java_timestream.invoke_arn
    native_java_mongodb_lambda_invoke_arn    = var.native_lambda_java_mongodb.invoke_arn
  }
}
````
To deploy our API Gateway with Terraform, we provide some configuration for the Stage deployment and base path mapping. Stage is an environment (in this case "dev") to deploy to. Base path mapping is used so that methods can be called via our own custom domain name.

```` javascript

resource "aws_api_gateway_deployment" "openapi_deployment" {
  rest_api_id = aws_api_gateway_rest_api.cloudia_api_gateway_openapi.id

  triggers = {
    redeployment = sha1(jsonencode(aws_api_gateway_rest_api.cloudia_api_gateway_openapi.body))
  }

  lifecycle {
    create_before_destroy = true
  }
}

resource "aws_api_gateway_stage" "openapi_stage" {
  deployment_id = aws_api_gateway_deployment.openapi_deployment.id
  rest_api_id   = aws_api_gateway_rest_api.cloudia_api_gateway_openapi.id
  stage_name    = "api"
}

resource "aws_api_gateway_base_path_mapping" "apigateway" {
  depends_on = [
    aws_api_gateway_deployment.openapi_deployment
  ]
  domain_name = aws_api_gateway_domain_name.apigateway.domain_name
  api_id      = aws_api_gateway_rest_api.cloudia_api_gateway_openapi.id
  stage_name  = aws_api_gateway_stage.openapi_stage.stage_name
  base_path   = "api"
}

````

Our OAD contains multiple sections. Most crucial for our discussion here is the 'paths' segment in which we describe the endpoints of our API and the associated methods (GET, POST, PUT, DELETE). As an example we have our endpoint `/timestream/betweenTwoTimestamp` with a `GET` method attached to it.

Using the `security` property we configure the endpoint to be set up according to our configuration in a specific Cognito User Pool. Via the `x-amazon-apigateway-integration` property we configure the API Gateway for this specific API endpoint.

```` javascript
  /timestream/betweenTwoTimestamp:
    get:
      security:
        - CognitoUserPool: [ "cloudia/all" ]
      summary: Fetch data between two specific timestamp
      x-amazon-apigateway-integration:
        type: HTTP_PROXY
        httpMethod: GET
        uri: 'https://${apprunner_url}/timestream/betweenTwoTimestamp'
        payloadFormatVersion: 2.0
      operationId: getBetweenTwoTimestampTimestream
      parameters:
        - name: serialMeterId
          in: query
          required: true
          description: Timestream serial meter id
          schema:
            type: string
            default: '4531323334353637343536'
        - name: first-timestamp
          in: query
          required: true
          description: Timestream query timestamp string
          schema:
            type: string
            default: '2024-01-10 13:08:15'
        - name: second-timestamp
          in: query
          required: true
          description: Timestream query timestamp string
          schema:
            type: string
            default: '2024-01-10 13:08:17'
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                type: array
                items:
                  $ref: '#/components/schemas/SmartMeterDataTimestream'
````

A few other properties are ommited from our screenshot; we use the `parameters` property to set the request parameters of the API and we use the `responses` property to define potential responses that can be received back by the end user.

For documentation and testing of the API we use the Swagger UI generated by our OAD as documentation. For the endpoint `/timestream/betweenTwoTimestamp` it looks like this:

<img class="p-image" src="{{ '/img/2024-03-04-cloudia/swaggerui.png' | prepend: site.baseurl }}" class="image fit" style="margin:0px auto; max-width: 30%;">


## Route 53

Route 53 is a `Domain Name System` (DNS) service. It lets you route requests to your domain. In our project it is used with the [api gateway](#apigateway-and-openapi). User's are able to make requests to the route 53 endpoint which will then forward it to the api gateway and resolve the request.

## Cognito

Cognito is an identity and access management service by AWS. This service provides `security` to our application (using Spring security) as well as for our AWS API Gateway. The user has to login in order to access these resources and thus provide an access token. There are multiple ways in order to retrieve the access token. In our project we chose the `Authorize Code Grant` flow. This flow will retrieve an authorization code first which then will be used to request an access token. In order to make requests to the AWS API Gateway and/or apprunner, this access token has to be provided in order to get access.

# Cloudia application

## OBIS code data

OBIS code data are standardized identifiers used by P1 meters (Dutch Digital Electricity Meter) to represent different types of energy consumption data. Each OBIS code identifier corresponds to a specific piece of information, such as electricity usage, meter status, or tariff information. These codes help organize and categorize the data collected by the meter. Below snippet code is an example:

````
0-0:96.1.4(xxxxx)
0-0:96.1.1(xxxxxxxxxxxxxxxxxxxxxxxxxxxx)
0-0:1.0.0(210204163628W)
1-0:1.8.1(000439.094kWh)
1-0:1.8.2(000435.292kWh)
1-0:2.8.1(000035.805kWh)
1-0:2.8.2(000012.156kWh)
0-0:96.14.0(0001)
````
For more details about OBISCODE, please refer to the following [Link](https://jensd.be/1205/linux/data-lezen-van-de-belgische-digitale-meter-met-de-p1-poort).

## Message converters

The data generated by the P1 Meter is in the form of OBIS code data, as previously explained. To improve readability, a script is needed to translate this data into a readable format. As a result, we have developed multiple Lambdas for this purpose, using Java, Python, and Native implementations. The objective is to benchmark these Lambdas to determine which one is the most optimal in terms of performance, price, speed, and relevance for our project. These Lambdas will receive OBIS code data and convert it into JSON formatted data. Afterwards, this data will be stored in a database, such as AWS Timestream or MongoDB. In the following sections, the process of creating these Lambdas will be explained.
### Java converter

Using Java, OBIS code data is converted into JSON format with the following steps.

Multiple methods were created to work together in converting OBIS code data to JSON format. The first method is mapAndSetValue(), which receives OBIS code data and maps it to the correct property of our POJO object (SmartMeterData).

```java
    public void mapAndSetValue(String obisCode, String value, SmartMeterData data) {
        DateTimeFormatter formatter = DateTimeFormatter.ofPattern("yyMMddHHmmssX");

        switch (obisCode) {
            case "0-0:96.1.4":
                data.setId(value);
                break;
            case "0-0:96.1.1":
                data.setSerialNumberElectricityMeter(value);
                break;
            case "0-0:1.0.0":
                String timestampString = value.substring(0, value.length() - 1) + "Z";
                Instant timestamp = Instant.from(formatter.parse(timestampString));
                data.setTimestamp(timestamp);
                break;
                // rest of the fields ...
```

The next method is processLine(). This method processes each OBIS code line and calls the mapAndSetValue() method to map it to the correct property of our POJO object.

```java
    public void processLine(String line, SmartMeterData data) {

        String[] parts = line.split("\\(", 2);
        if (parts.length == 2) {
            String obisCode = parts[0].trim();
            String value = parts[1].split("\\)", 2)[0];

            MapAndSetValue mapAndSetValue = new MapAndSetValue();
            mapAndSetValue.mapAndSetValue(obisCode, value, data);
        }
    }
```

Lastly the convertRawSmartMeterDatasToSmartMeterDataList method takes the OBIS code data as input. It executes both processLine() and mapAndSetValue() to accomplish the conversion. Ultimately, it returns the OBIS data in JSON format.

```java
    public List<SmartMeterData> convertRawSmartMeterDatasToSmartMeterDataList(String rawSmartMeterData) throws IOException {
        BufferedReader reader = new BufferedReader(new StringReader(rawSmartMeterData));
        String line;
        SmartMeterData smartMeterData = new SmartMeterData();
        ProcessLine processLine = new ProcessLine();

        while ((line = reader.readLine()) != null) {
            if (line.startsWith("0-0:96.1.4")) {
                // If a new timestamp starts, create a new SmartMeterData object
                smartMeterData = new SmartMeterData();
                smartMeterDataList.add(smartMeterData);
            }
            processLine.processLine(line, smartMeterData);
        }

        return smartMeterDataList;
    }
```
### Native image Java (GraalVM) converter

A native image is a tool that compiles the code ahead-of-time. This means it will try to compile everything ahead-of-time instead of runtime. It also creates an executable instead of a jar file when compiling the code. It can make the application faster because the compilation happens ahead-of time which enhances the user experience. However, it does come with some cons aswell. The compiling of the application takes much longer then without the use of native images. Because it's compiles ahead-of-time, classes that use reflection for example, can also cause problems. So there are sometimes compatability issues with it which can bring more complexity to the code.

### Python converter

We've also developed a Python script for conversion. Python is more straightforward as we only need a script to process the conversion.

This method takes the OBIS code data as input, converts it, and returns it as an object. First, an object is created to represent the OBIS code data. Then, a business logic is implemented to map each line of OBIS code to the correct property of the object.

```python
def convert_generated_object(generatedObjects):
    lines = generatedObjects.split('\n')

    dataObject = {}

    for line in lines:
        tokens = line.split('(')
        key = tokens[0].strip()

        if key == '1-0:1.6.0':
            value = tokens[2].rstrip(')')
            dataObject['maximumDemandLast1Month'] = parse_complex_value(value)

        elif len(tokens) == 2:
            key = tokens[0].strip()
            value = tokens[1].rstrip(')')
            
            if key == '0-0:1.0.0':
                timestamp_str = value.replace('W', 'Z')
                input_format = '%y%m%d%H%M%S%z'  
                timestamp_datetime = datetime.strptime(timestamp_str, input_format).isoformat()               
                dataObject['timestamp'] = timestamp_datetime           
            elif key == '0-0:96.1.4':
                dataObject['meterId'] = value                           
            elif key == '0-0:96.1.1':
                dataObject['serialNumberElectricityMeter'] = value
            ....
```

## Benchmarking converters

Multiple Lambdas have been created, and benchmarking on each Lambda has been performed to determine which is the most optimal in terms of performance, price, speed, and relevance for our project. Each Lambda was run separately, and the following images display the results of the invocations for each one.

Below are the results for the Java Lambda Converter.

Foto

Below are the results for the Pyhton Lambda Converter.

Foto

Below are the results for the Native Lambda Converter.

Foto

In summary, the Python Lambda converter has the best performance in speed, memory usage, and billing efficiency.
Following closely is the Native Lambda converter, which also demonstrates good overall performance but falls slightly short compared to the Python Lambda.
Lastly, the Java Lambda converter ranks lowest in overall performance compared to the other options.

## Github Actions

We don't want to manually deploy our application and manually run terraform. This whole process is automated using Github Actions workflows. These are CI/CD pipelines. These pipelines consists of one or more jobs. Jobs are a set of steps that are going to be executed. Let's take a look at some of these workflows we have configured.

### Terraform run/publish application workflow

This workflow has two jobs. In the first job, a native image is build and Terraform is runned afterwards which will create all the resources. In the second job, it will dockerize the application and push it to ECR. 

FIRST JOB:
```` javascript
    steps:
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - uses: actions/checkout@f43a0e5ff2bd294095638e18286ca9a3d1956744 # v3.6.0
      - name: Set up JDK 17
        uses: actions/setup-java@0ab4596768b603586c0de567f2430c30f5b0d2b0 # v3.13.0
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'liberica'
          cache: maven

      - name: Maven package
        run: mvn clean package
        working-directory: ""

      - uses: graalvm/setup-graalvm@v1
        if: ${{ (github.event_name == 'push' && github.ref == 'refs/heads/main') || ( github.event_name == 'schedule' && github.ref == 'refs/heads/main' )}}
        with:
          java-version: '17.0.7'
          distribution: 'graalvm'

      - name: Maven package lambda native
        if: ${{ (github.event_name == 'push' && github.ref == 'refs/heads/main') || ( github.event_name == 'schedule' && github.ref == 'refs/heads/main' )}}
        run: |
          cd ../lambdas
          chmod +x bootstrap
          mvn -Pnative native:compile

      - name: Zip for native lambda
        if: ${{ (github.event_name == 'push' && github.ref == 'refs/heads/main') || ( github.event_name == 'schedule' && github.ref == 'refs/heads/main' )}}
        run: |
          cd ../lambdas
          zip -j native.zip target/lambdas ./bootstrap

      - name: Install aws cli
        uses: unfor19/install-aws-cli-action@3c53dab4dd62b5d9d647f0ce9519285250a3c767 # v1.0.6

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@5fd3084fc36e372ff1fff382a39b10d03659f355 # v2
        with:
          role-to-assume: ${{ env.AWS_ROLE }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Terraform Init
        id: init
        run: terraform init

      - name: Terraform Format
        id: fmt
        run: terraform fmt -check

      - name: Terraform Validate
        id: validate
        run: terraform validate

      - name: Terraform Plan
        id: plan
        run: |
          terraform plan 

      - name: Terraform Apply
        id: apply
        run: |
          terraform apply -auto-approve \
            -var="telegram_bot_token=${{ secrets.TELEGRAM_BOT_AWS_TOKEN }}" \
            -var="telegram_chat_id=${{ secrets.TELEGRAM_CHAT_ID }}" \
            -var="mongodb_password=${{ secrets.mongodb_password }}"
        env:
          TF_telegram_bot_token: ${{ secrets.TELEGRAM_BOT_AWS_TOKEN }}
          TF_telegram_chat_id: ${{ secrets.TELEGRAM_CHAT_ID }}
````


SECOND JOB:
```` javascript
    steps:
      - uses: actions/checkout@f43a0e5ff2bd294095638e18286ca9a3d1956744 # v3.6.0
      - name: Set up JDK 17
        uses: actions/setup-java@0ab4596768b603586c0de567f2430c30f5b0d2b0 # v3.13.0
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'liberica'
          cache: maven

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@5fd3084fc36e372ff1fff382a39b10d03659f355 # v2
        with:
          role-to-assume: ${{ env.AWS_ROLE }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@2f9f10ea3fa2eed41ac443fee8bfbd059af2d0a4 # v1.6.0

      - name: Build with Maven and Docker tag
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          ECR_REPOSITORY: cloudia
        run: |
          ./mvnw clean install -Pprod
          cd application
          ./mvnw clean package -Pprod
          ./mvnw spring-boot:build-image -Pprod -Dspring-boot.build-image.imageName=$ECR_REGISTRY/$ECR_REPOSITORY:${{github.run_number}}
          
          docker tag $ECR_REGISTRY/$ECR_REPOSITORY:${{github.run_number}} $ECR_REGISTRY/$ECR_REPOSITORY:latest

      - name: Push image to ECR
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          ECR_REPOSITORY: cloudia
        run: |
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:latest
          docker push $ECR_REGISTRY/$ECR_REPOSITORY:${{github.run_number}}
````


Now we also want to deploy and provision these AWS resources at work hours. This is done using scheduler. In the following screenshot an example is shown how to configure the scheduler on the workflow. 

```` javascript

jobs:
  terraform_destroy_reschedule:
    runs-on: ubuntu-latest

    permissions:
      actions: read
      contents: read
      packages: write
      id-token: write

    outputs:
      conclusion: ${{ steps.cancel.outputs.should_cancel }}

    steps:
      - name: Download artifact
        id: download-artifact
        uses: dawidd6/action-download-artifact@268677152d06ba59fcec7a7f0b5d961b6ccd7e1e # v2
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          workflow: dynamic-provision-delete.yml
          workflow_conclusion: ""
          name: conclusion-artifact
          path: conclusion.txt
          search_artifacts: true

      - name: Debug Output
        id: cancel
        run: |
          cat conclusion.txt/conclusion.txt
          should_cancel=$(cat conclusion.txt/conclusion.txt)
          echo "::set-output name=should_cancel::$should_cancel" 
          echo $should_cancel

  terraform_destroy:
    runs-on: ubuntu-latest
    needs: terraform_destroy_reschedule

    if: ${{ needs.terraform_destroy_reschedule.outputs.conclusion == 'Canceling the terraform destroy procedure.' }}

    permissions:
      contents: read
      id-token: write

    defaults:
      run:
        working-directory: ${{ env.TF_DIR }}

    steps:
      - uses: actions/checkout@f43a0e5ff2bd294095638e18286ca9a3d1956744 # v3.6.0

      - name: Send Telegram Reminder
        env:
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
          TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_CHAT_ID }}
        run: |
          curl -s -X POST https://api.telegram.org/bot$TELEGRAM_BOT_TOKEN/sendMessage \
            -d chat_id=$TELEGRAM_CHAT_ID \
            -d text="As we near the end of the day, we'll proceed with the terraform destroy process. 
          Please ensure all your changes are saved. Your hard work is appreciated. Thank you!"

      - name: Wait for termination
        run: sleep 2m

      - name: Install aws cli
        uses: unfor19/install-aws-cli-action@3c53dab4dd62b5d9d647f0ce9519285250a3c767 # v1.0.6

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@5fd3084fc36e372ff1fff382a39b10d03659f355 # v2
        with:
          role-to-assume: ${{ env.AWS_ROLE }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Send Telegram Confirmation
        env:
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
          TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_CHAT_ID }}
        run: |
          curl -s -X POST https://api.telegram.org/bot$TELEGRAM_BOT_TOKEN/sendMessage \
            -d chat_id=$TELEGRAM_CHAT_ID \
            -d text="Terraform destroy initiated."

      - name: Terraform Init
        id: init
        run: terraform init

      - name: Terraform Destroy
        id: destroy
        run: terraform destroy -auto-approve
        continue-on-error: true

````


### Delete resources 

At the end on the workday, we want to delete all the resources we had created. Ofcourse on a production environment, this wouldn't be done. But to be efficient with our resources we delete them at the end of the workday. It is possible that someone works late in the evening on the project, which is why we have configured a Telegram notification. Before destroying the resources, a message/question is sent to a telegram channel asking if it's okay to destroy the resources. If it doesn't get any response, the resources get destroyed. If someone responds to the message with a "no", the deletion gets delayed with 2 hours.

```` javascript
jobs:
  terraform_voting:
    runs-on: ubuntu-latest

    outputs:
      conclusion: ${{ steps.voting.outputs.response }}

    steps:
      - name: Checkout code
        uses: actions/checkout@ee0669bd1cc54295c223e0bb666b733df41de1c5 # v2.7.0

      - name: Install dependencies
        run: pip install python-telegram-bot==13.13

      - name: Run Telegram Voting Script
        id: voting
        run: |
          response=$(python .github/scripts/closeOff.py)
          echo "::set-output name=response::$response"
          echo $response
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
          TELEGRAM_BOT_TOKEN_API: ${{ secrets.TELEGRAM_BOT_TOKEN_API }}
          TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_CHAT_ID }}

      - name: Store Conclusion as Artifact
        run: echo "${{ steps.voting.outputs.response }}" > conclusion.txt
      - uses: actions/upload-artifact@a8a3f3ad30e3422c9c7b888a15615d19a852ae32 # v3
        with:
          name: conclusion-artifact
          path: conclusion.txt

      - name: Echo Conclusion
        run: cat conclusion.txt

  destroy:
    runs-on: ubuntu-latest
    needs: terraform_voting
    timeout-minutes: 15

    if: ${{ needs.terraform_voting.outputs.conclusion == 'Proceeding with terraform destroy.' }}

    permissions:
      contents: read
      id-token: write

    defaults:
      run:
        working-directory: ${{ env.TF_DIR }}

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - uses: actions/checkout@f43a0e5ff2bd294095638e18286ca9a3d1956744 # v3.6.0

      - name: Install aws cli
        uses: unfor19/install-aws-cli-action@3c53dab4dd62b5d9d647f0ce9519285250a3c767 # v1.0.6

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@5fd3084fc36e372ff1fff382a39b10d03659f355 # v2
        with:
          role-to-assume: ${{ env.AWS_ROLE }}
          aws-region: ${{ env.AWS_REGION }}
      - name: Send Telegram Reminder
        env:
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
          TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_CHAT_ID }}
        run: |
          curl -s -X POST https://api.telegram.org/bot$TELEGRAM_BOT_TOKEN/sendMessage \
            -d chat_id=$TELEGRAM_CHAT_ID \
            -d text="We'll proceed with the terraform destroy process. 
          Your hard work is appreciated. Thank you!"

      - name: Terraform Init
        id: init
        run: terraform init

      - name: Terraform Destroy
        id: destroy
        run: terraform destroy -auto-approve
        continue-on-error: true


````


### Infracost workflow

Another feature that has been added is the implementation of Infracost. Infracost is a tool that can be used to monitor the cost of the resources which are going to be created with Terraform. It compares the previous Terraform state with the current one. Whenever a pull request is made to the production environment, the infracost workflow will run and provide comment a report of the changes in the infrastructure cost that are going to happen if applied.

In the following code, you can see the implementation of infracost.

```yaml
      # Generate Infracost JSON file as the baseline.
      - name: Generate Infracost cost estimate baseline
        run: |
          export INFRACOST_API_KEY=${{ secrets.INFRACOST_API_KEY }}
          cd ${TF_ROOT}
          infracost breakdown --path . --format json --out-file infracost-base.json

      # Generate an Infracost diff and save it to a JSON file.
      - name: Generate Infracost diff
        run: |
          export INFRACOST_API_KEY=${{ secrets.INFRACOST_API_KEY }}
          cd ${TF_ROOT}
          infracost diff --path ./ --compare-to infracost-base.json

      # generate the html report based on the JSON output from last step
      - name: Generate Infracost Report
        run: |
          export INFRACOST_API_KEY=${{ secrets.INFRACOST_API_KEY }}
          cd ${TF_ROOT}
          infracost breakdown --path . --format json --out-file infracost-base.json


      - name: Post Infracost comment
        run: |
          export INFRACOST_API_KEY=${{ secrets.INFRACOST_API_KEY }}
          infracost comment github --path=./infracost-base.json \
            --repo=$GITHUB_REPOSITORY \
            --pull-request=$PR_NUMBER \
            --commit=$GITHUB_SHA \
            --github-token=${{ secrets.GITHUB_TOKEN }} \
            --behavior=update
```
 

 The comment on the PR looks as follows:

<img class="p-image" src="{{ '/img/2024-03-04-cloudia/infracost.png' | prepend: site.baseurl }}" class="image fit" style="margin:0px auto; max-width: 30%;">

## End Result (****)

A P1 digital electricity meter is being read and sent to an MQTT topic. We have multiple lambda's that gets triggered when a new message (data) is sent to this topic. The lambda's will convert and save it to the database. The reason why we have multiple

The end result of our application that reads electricity consumption and production via the P1 port of an electrical meter installed in the home of a colleague. These readings are processed by a Raspberry Pi which uses AWS IoT Core to push the data to the cloud in a Base64 format.

The data is then processed by a lambda function which decodes the base64 input into the original Obiscode syntax from the digital meter; to then convert this data into legible segments that can be saved into a database.

The second part of our project is comprised of fetching the data that we saved into our database. For this we provided an authorization via AWS Cognito which connects to an App Runner that uses Spring Boot code to read from the database and process the queries we request. Using valid credentials one has access to this data.

We tested using Gatling to make requests and see how long it takes before the requests start failing when we keep applying stress on our application. 
